# Testing Directory

This directory contains all files for comparative LLM testing. After testing is complete, you can easily remove this entire directory to clean up.

---

## ğŸ“ Directory Structure

```
testing/
â”œâ”€â”€ README.md                          # This file
â”‚
â”œâ”€â”€ Core Scripts
â”œâ”€â”€ test_harness.py                    # Main test execution script
â”œâ”€â”€ analyze_results.py                 # Results analysis and visualization
â”œâ”€â”€ verify_setup.py                    # Pre-flight checks
â”œâ”€â”€ extract_ground_truth.py            # Ground truth data extractor
â”‚
â”œâ”€â”€ Configuration
â”œâ”€â”€ test_config.json                   # Test configuration
â”‚
â”œâ”€â”€ Question Bank
â”œâ”€â”€ test_question_bank.json            # 50 test questions
â”œâ”€â”€ test_question_bank.csv             # CSV version
â”œâ”€â”€ test_results_template.csv          # Manual scoring template
â”‚
â”œâ”€â”€ Dependencies
â”œâ”€â”€ requirements_testing.txt           # Python dependencies
â”‚
â”œâ”€â”€ Documentation
â”œâ”€â”€ QUICKSTART.md                      # Quick start guide
â”œâ”€â”€ TEST_HARNESS_USAGE.md              # Complete usage docs
â”œâ”€â”€ COMPARATIVE_TESTING_GUIDE.md       # Testing methodology
â”œâ”€â”€ QUESTION_BANK_SUMMARY.md           # Coverage analysis
â”œâ”€â”€ LM_STUDIO_SETUP.md                 # LM Studio setup guide
â”‚
â””â”€â”€ Results (generated during testing)
    â””â”€â”€ test_results/                  # Test outputs and analysis
```

---

## ğŸš€ Quick Start (3 Steps)

### 1. Install Dependencies

```bash
cd testing
pip install -r requirements_testing.txt
```

### 2. Verify Setup

```bash
python verify_setup.py
```

Expected output:
```
âœ… SETUP VERIFICATION PASSED!
```

### 3. Run Pilot Test

**Terminal 1 - Start ClimateGPT:**
```bash
cd ..
make serve
```

**Terminal 2 - Run Test:**
```bash
cd testing
python test_harness.py --pilot
```

---

## ğŸ“Š What Gets Tested

- **50 questions** covering all ClimateGPT capabilities
- **8 sectors**: transport, power, waste, agriculture, buildings, fuel-exploitation, industrial-combustion, industrial-processes
- **3 levels**: country, admin1 (states), city
- **2 temporal grains**: yearly, monthly
- **4 question types**: simple, temporal, comparative, complex

---

## ğŸ¯ Testing Goals

1. **Accuracy**: Compare ClimateGPT vs Meta Llama
2. **Performance**: Response time benchmarks
3. **Quality**: Response formatting and clarity
4. **Coverage**: Test all sectors, levels, and grains
5. **Improvements**: Identify areas to enhance

---

## ğŸ“š Documentation Guide

| File | Purpose | Read When |
|------|---------|-----------|
| **QUICKSTART.md** | Quick 5-minute start | First time setup |
| **TEST_HARNESS_USAGE.md** | Complete reference | Running tests |
| **LM_STUDIO_SETUP.md** | LM Studio setup | Installing Llama |
| **COMPARATIVE_TESTING_GUIDE.md** | Testing methodology | Understanding approach |
| **QUESTION_BANK_SUMMARY.md** | Question coverage | Reviewing test scope |

---

## ğŸ› ï¸ Common Commands

### Testing

```bash
# Pilot test (10 questions, ~2 min)
python test_harness.py --pilot

# Full test (50 questions, ~20 min)
python test_harness.py

# Test only ClimateGPT (no Llama needed)
python test_harness.py --climategpt-only

# Test specific questions
python test_harness.py --questions 1,2,3,4,5

# Verbose output
python test_harness.py --pilot --verbose
```

### Analysis

```bash
# Basic analysis
python analyze_results.py

# With visualizations
python analyze_results.py --visualize

# Export report
python analyze_results.py --report

# All at once
python analyze_results.py --visualize --report
```

### Verification

```bash
# Check setup
python verify_setup.py

# Test LM Studio connection
curl http://localhost:1234/v1/models
```

---

## âš™ï¸ Configuration

Edit `test_config.json`:

```json
{
  "climategpt": {
    "url": "http://localhost:8010"
  },
  "llama": {
    "url": "http://localhost:1234",
    "model": "meta-llama-3.1-8b-instruct@q4_k_m"
  }
}
```

**Note**: Model ID must match what's in LM Studio (check with `curl http://localhost:1234/v1/models`)

---

## ğŸ“ˆ Expected Results

**ClimateGPT**:
- âœ… High accuracy (real data from DuckDB)
- âœ… Specific numbers with units
- âœ… Source attribution
- Response time: 1000-2000ms average

**Meta Llama**:
- âŒ Low accuracy (no real data access)
- âŒ Generic or hallucinated responses
- âœ… Natural conversational tone
- Response time: 500-1000ms average

---

## ğŸ› Troubleshooting

### ClimateGPT Not Running

```bash
# From project root
cd ..
make serve
```

### LM Studio Not Responding

1. Open LM Studio app
2. Go to "Local Server" tab
3. Verify model is loaded
4. Click "Start Server"
5. Test: `curl http://localhost:1234/v1/models`

### Import Errors

```bash
pip install -r requirements_testing.txt
```

### Wrong Model ID

Check actual model ID:
```bash
curl http://localhost:1234/v1/models | jq '.data[0].id'
```

Update in `test_config.json`:
```json
{
  "llama": {
    "model": "your-actual-model-id-here"
  }
}
```

---

## ğŸ—‘ï¸ Cleanup After Testing

Once testing is complete and you have your results:

```bash
# From project root
cd ..
rm -rf testing/

# Or keep results and docs, remove only scripts
cd testing
rm test_harness.py analyze_results.py verify_setup.py
```

---

## ğŸ“¦ Files You Can Safely Delete

After testing, you can remove:

**Immediately** (if you don't need to re-run):
- `test_harness.py`
- `analyze_results.py`
- `verify_setup.py`
- `extract_ground_truth.py`
- `requirements_testing.txt`

**Keep for Reference**:
- `test_results/` - Your test results
- `test_question_bank.json` - Question bank
- Documentation files - For future reference

**Or Remove Everything**:
```bash
cd ..
rm -rf testing/
```

---

## ğŸ“ Next Steps After Testing

1. âœ… Review results in `test_results/`
2. âœ… Identify top 5 improvement areas
3. âœ… Implement improvements in ClimateGPT source
4. âœ… Re-test to verify improvements
5. âœ… Document findings

---

## ğŸ“ Getting Help

1. Check `QUICKSTART.md` for quick issues
2. Review `TEST_HARNESS_USAGE.md` for detailed docs
3. Run `python verify_setup.py` to check configuration
4. Check error messages carefully
5. Verify both services are running

---

## âœ… Status

- âœ… All scripts ready to use
- âœ… Configuration updated for your LM Studio models
- âœ… Documentation complete
- âœ… Ready to test!

---

**Start here**: Read [QUICKSTART.md](QUICKSTART.md) for 5-minute setup
**Configuration**: [test_config.json](test_config.json)
**Run test**: `python test_harness.py --pilot`

**Created**: 2025-11-02
**Status**: Production-ready âœ…
